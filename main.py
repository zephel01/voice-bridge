#!/usr/bin/env python3
"""
Voice Bridge - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è‹±æ—¥ç¿»è¨³ã‚¢ãƒ—ãƒª
YouTubeã®è‹±èªéŸ³å£°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ—¥æœ¬èªéŸ³å£°ã«ç¿»è¨³ã™ã‚‹

ä½¿ã„æ–¹:
  python main.py          # GUI ãƒ¢ãƒ¼ãƒ‰ï¼ˆWhisperï¼‰
  python main.py --asr moonshine  # Moonshine ã§èµ·å‹•
  python main.py --cli    # CLI ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
  python main.py --list-devices  # å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ä¸€è¦§ã‚’è¡¨ç¤º
"""

import argparse
import os
import platform
import sys
import threading
import signal
import time

# OS ã«å¿œã˜ãŸ AudioCapture ã‚’é¸æŠ
IS_WINDOWS = platform.system() == "Windows"

if IS_WINDOWS:
    from audio_capture_win import WindowsAudioCapture as AudioCapture
    DEFAULT_DEVICE = "default"
else:
    from audio_capture import AudioCapture
    DEFAULT_DEVICE = "BlackHole 2ch"

# ASR ã‚¨ãƒ³ã‚¸ãƒ³ã¯ --asr ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§é¸æŠï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: whisperï¼‰
# main() ã® argparse ã§åˆ‡ã‚Šæ›¿ãˆã€VoiceBridge ã«æ³¨å…¥ã™ã‚‹
from transcriber import Transcriber as WhisperTranscriber
from translator import Translator
from tts_engine import TTSEngine
from tts_voicevox import VoicevoxTTS
from player import AudioPlayer
from translation_logger import TranslationLogger
from ai_chat import AiChat, load_dotenv

# .env ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()


class VoiceBridge:
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        device_name: str = DEFAULT_DEVICE,
        model_size: str = "small",
        source_language: str = "en",
        target_language: str = "ja",
        tts_language: str = None,
        voice: str = "nanami",
        chunk_duration: float = 4.0,
        use_voicevox: bool = False,
        voicevox_speaker_id: int = 3,
        asr_engine: str = "whisper",
        mode: str = "translate",
        ai_base_url: str = "https://api.openai.com/v1",
        ai_api_key: str = None,
        ai_model: str = "gpt-4o-mini",
    ):
        # TTSè¨€èªã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç¿»è¨³è¨€èªã¨åŒã˜
        if tts_language is None:
            tts_language = target_language

        self.source_language = source_language
        self.target_language = target_language
        self.tts_language = tts_language

        self.asr_engine = asr_engine
        self.capture = AudioCapture(
            device_name=device_name,
            chunk_duration=chunk_duration,
        )

        # ASR ã‚¨ãƒ³ã‚¸ãƒ³ã®é¸æŠ
        if asr_engine == "moonshine":
            from transcriber_moonshine import Transcriber as MoonshineTranscriber
            self.transcriber = MoonshineTranscriber(model_size=model_size, language=source_language)
            print(f"[VoiceBridge] ASR: Moonshine (language={source_language})")
        else:
            self.transcriber = WhisperTranscriber(model_size=model_size, language=source_language)
            print(f"[VoiceBridge] ASR: faster-whisper (model={model_size}, language={source_language})")
        # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯ç¿»è¨³ä¸è¦
        if mode != "chat":
            self.translator = Translator(source=source_language, target=target_language)
        else:
            self.translator = None

        # TTS ã‚¨ãƒ³ã‚¸ãƒ³: VOICEVOX ãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ãã¡ã‚‰ã‚’ä½¿ã†ï¼ˆãŸã ã—æ—¥æœ¬èªã®ã¿å¯¾å¿œï¼‰
        self.use_voicevox = use_voicevox
        self._voicevox_speaker_id = voicevox_speaker_id
        if use_voicevox and tts_language == "ja":
            self.tts = VoicevoxTTS(speaker_id=voicevox_speaker_id)
            print(f"[VoiceBridge] TTS: VOICEVOX (speaker_id={voicevox_speaker_id})")
        else:
            if use_voicevox and tts_language != "ja":
                print(f"[VoiceBridge] VOICEVOX ã¯æ—¥æœ¬èªã®ã¿å¯¾å¿œã®ãŸã‚ã€Edge TTS ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            self.tts = TTSEngine(language=tts_language, voice=voice)
            print(f"[VoiceBridge] TTS: Edge TTS (language={tts_language})")

        self.player = AudioPlayer()
        self.logger = TranslationLogger(log_dir="logs")

        # ãƒ¢ãƒ¼ãƒ‰: "translate"ï¼ˆç¿»è¨³ï¼‰or "chat"ï¼ˆAIä¼šè©±ï¼‰
        self.mode = mode
        self.ai_chat = None
        if mode == "chat":
            self.ai_chat = AiChat(
                base_url=ai_base_url,
                api_key=ai_api_key,
                model=ai_model,
                response_language=tts_language or target_language,
            )
            print(f"[VoiceBridge] ãƒ¢ãƒ¼ãƒ‰: AI ãƒãƒ£ãƒƒãƒˆ")
        else:
            print(f"[VoiceBridge] ãƒ¢ãƒ¼ãƒ‰: ç¿»è¨³")

        self._running = False
        self._pipeline_thread = None
        self._is_playing = False  # TTSå†ç”Ÿä¸­ãƒ•ãƒ©ã‚°ï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰

        # GUI ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨
        self.on_english_text = None
        self.on_japanese_text = None
        self.on_status_change = None
        self.on_level = None       # (rms: float, is_active: bool)
        self.on_latency = None     # (latency_sec: float, stage: str)

        # éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ AudioCapture ã«æ¥ç¶š
        self.capture.on_level = self._on_capture_level

        # å†ç”ŸçŠ¶æ…‹ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ AudioPlayer ã«æ¥ç¶šï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
        self.player.on_play_start = self._on_play_start
        self.player.on_play_end = self._on_play_end

    def _on_capture_level(self, rms: float, is_active: bool):
        """AudioCapture ã‹ã‚‰ã®ãƒ¬ãƒ™ãƒ«é€šçŸ¥ã‚’ä¸­ç¶™"""
        if self.on_level:
            self.on_level(rms, is_active)

    def _on_play_start(self):
        """TTS å†ç”Ÿé–‹å§‹æ™‚ â€” ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’æŠ‘åˆ¶"""
        self._is_playing = True
        print("[VoiceBridge] TTSå†ç”Ÿé–‹å§‹ â†’ ã‚­ãƒ£ãƒ—ãƒãƒ£æŠ‘åˆ¶")

    def _on_play_end(self):
        """TTS å†ç”Ÿçµ‚äº†æ™‚ â€” ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’å†é–‹ï¼ˆå°‘ã—å¾…ã£ã¦ãƒãƒƒãƒ•ã‚¡ã«æ®‹ã‚‹TTSéŸ³å£°ã‚’æ¨ã¦ã‚‹ï¼‰"""
        # å†ç”Ÿçµ‚äº†ç›´å¾Œã®ãƒãƒƒãƒ•ã‚¡ã«TTSéŸ³å£°ã®æ®‹ã‚ŠãŒå…¥ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§å°‘ã—å¾…ã¤
        time.sleep(0.3)
        # ãƒãƒƒãƒ•ã‚¡ã«æºœã¾ã£ãŸãƒãƒ£ãƒ³ã‚¯ã‚’æ¨ã¦ã‚‹
        while not self.capture.audio_queue.empty():
            try:
                self.capture.audio_queue.get_nowait()
            except Exception:
                break
        self._is_playing = False
        print("[VoiceBridge] TTSå†ç”Ÿçµ‚äº† â†’ ã‚­ãƒ£ãƒ—ãƒãƒ£å†é–‹")

    def _notify_latency(self, latency: float, stage: str):
        """é…å»¶æƒ…å ±ã‚’é€šçŸ¥"""
        if self.on_latency:
            self.on_latency(latency, stage)

    def _pipeline_loop(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ï¼ˆãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦åˆ†å²ï¼‰"""
        if self.mode == "chat":
            self._chat_pipeline_loop()
        else:
            self._translate_pipeline_loop()

    def _translate_pipeline_loop(self):
        """ç¿»è¨³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        self._notify_status("ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
        self.transcriber.load_model()
        self._notify_status("ã‚­ãƒ£ãƒ—ãƒãƒ£ä¸­...")

        while self._running:
            # 1. éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            audio_chunk = self.capture.get_chunk(timeout=1.0)
            if audio_chunk is None:
                continue

            # TTS å†ç”Ÿä¸­ã¯ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’æ¨ã¦ã‚‹ï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
            if self._is_playing:
                print("[Pipeline] TTSå†ç”Ÿä¸­ã®ãŸã‚éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                continue

            t_start = time.time()
            self._notify_status("èªè­˜ä¸­...")

            # 2. éŸ³å£°èªè­˜ï¼ˆè‹±èªãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼‰
            t_step = time.time()
            try:
                english_text = self.transcriber.transcribe(audio_chunk)
            except Exception as e:
                print(f"[Pipeline] éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                continue
            t_transcribe = time.time() - t_step

            if not english_text.strip():
                self._notify_status("ã‚­ãƒ£ãƒ—ãƒãƒ£ä¸­...")
                continue

            source_label = self.source_language.upper()
            print(f"[{source_label}] {english_text}")
            if self.on_english_text:
                self.on_english_text(english_text)

            # 3. ç¿»è¨³
            self._notify_status("ç¿»è¨³ä¸­...")
            t_step = time.time()
            try:
                translated_text = self.translator.translate(english_text)
            except Exception as e:
                print(f"[Pipeline] ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
                continue
            t_translate = time.time() - t_step

            if not translated_text.strip():
                self._notify_status("ã‚­ãƒ£ãƒ—ãƒãƒ£ä¸­...")
                continue

            target_label = self.target_language.upper()
            print(f"[{target_label}] {translated_text}")
            if self.on_japanese_text:
                self.on_japanese_text(translated_text)

            # ãƒ­ã‚°ä¿å­˜
            self.logger.log(
                self.source_language, self.target_language,
                english_text, translated_text,
            )

            # 4. éŸ³å£°åˆæˆ
            self._notify_status("éŸ³å£°åˆæˆä¸­...")
            t_step = time.time()
            try:
                audio_path = self.tts.synthesize(translated_text)
            except Exception as e:
                print(f"[Pipeline] TTS ã‚¨ãƒ©ãƒ¼: {e}")
                continue
            t_tts = time.time() - t_step

            if audio_path:
                self.player.enqueue(audio_path)

            t_total = time.time() - t_start
            # ãƒãƒ£ãƒ³ã‚¯è“„ç©æ™‚é–“ã‚‚åŠ ç®—ã—ãŸå®Ÿè³ªé…å»¶
            total_with_chunk = t_total + self.capture.chunk_duration
            print(f"[Latency] èªè­˜={t_transcribe:.1f}s ç¿»è¨³={t_translate:.1f}s TTS={t_tts:.1f}s "
                  f"å‡¦ç†è¨ˆ={t_total:.1f}s å®Ÿè³ªé…å»¶={total_with_chunk:.1f}s")
            self._notify_latency(total_with_chunk,
                f"èªè­˜{t_transcribe:.1f}s+ç¿»è¨³{t_translate:.1f}s+TTS{t_tts:.1f}s")

            self._notify_status("ã‚­ãƒ£ãƒ—ãƒãƒ£ä¸­...")

    def _chat_pipeline_loop(self):
        """AI ãƒãƒ£ãƒƒãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ï¼ˆãƒã‚¤ã‚¯å…¥åŠ›ï¼‰"""
        print("")
        print("[1/4] ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
        self._notify_status("ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
        self.transcriber.load_model()
        print("[1/4] ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† âœ“")
        print("[====] ãƒã‚¤ã‚¯å¾…æ©Ÿä¸­... è©±ã—ã‹ã‘ã¦ãã ã•ã„")
        self._notify_status("ãƒã‚¤ã‚¯å¾…æ©Ÿä¸­...")

        # ç™ºè©±ãƒãƒƒãƒ•ã‚¡: ç„¡éŸ³ãŒç¶šãã¾ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æºœã‚ã‚‹
        utterance_buffer = []
        silence_count = 0
        SILENCE_THRESHOLD = 2  # ç„¡éŸ³ãƒãƒ£ãƒ³ã‚¯ãŒé€£ç¶šNå›ã§ç™ºè©±çµ‚äº†ã¨åˆ¤å®š

        while self._running:
            # 1. éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            audio_chunk = self.capture.get_chunk(timeout=1.0)
            if audio_chunk is None:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ = ç„¡éŸ³æ‰±ã„
                if utterance_buffer:
                    silence_count += 1
                    if silence_count >= SILENCE_THRESHOLD:
                        user_text = " ".join(utterance_buffer)
                        utterance_buffer.clear()
                        silence_count = 0
                        self._chat_send_to_ai(user_text)
                continue

            # TTS å†ç”Ÿä¸­ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
            if self._is_playing:
                continue

            # 2. éŸ³å£°èªè­˜ï¼ˆãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼‰
            if not utterance_buffer:
                print("")
                print("[1/4] éŸ³å£°èªè­˜ä¸­...")
            self._notify_status("èªè­˜ä¸­...")
            t_step = time.time()
            try:
                chunk_text = self.transcriber.transcribe(audio_chunk)
            except Exception as e:
                print(f"[1/4] éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                continue
            t_transcribe = time.time() - t_step

            if not chunk_text.strip():
                # ç„¡éŸ³ãƒãƒ£ãƒ³ã‚¯ â†’ ãƒãƒƒãƒ•ã‚¡ã«æºœã¾ã£ã¦ã„ã‚Œã°ç™ºè©±çµ‚äº†åˆ¤å®š
                if utterance_buffer:
                    silence_count += 1
                    if silence_count >= SILENCE_THRESHOLD:
                        user_text = " ".join(utterance_buffer)
                        utterance_buffer.clear()
                        silence_count = 0
                        self._chat_send_to_ai(user_text)
                    else:
                        print(f"[1/4] (ç„¡éŸ³ {silence_count}/{SILENCE_THRESHOLD}...)")
                continue

            # éŸ³å£°ã‚ã‚Š â†’ ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ã€ç„¡éŸ³ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
            silence_count = 0
            utterance_buffer.append(chunk_text.strip())
            print(f"[1/4] èªè­˜: \"{chunk_text.strip()}\" (ãƒãƒƒãƒ•ã‚¡: {len(utterance_buffer)}ä»¶)")
            self._notify_status(f"èã„ã¦ã¾ã™... ({len(utterance_buffer)})")

    def _chat_send_to_ai(self, user_text: str):
        """ãƒãƒƒãƒ•ã‚¡ã«æºœã¾ã£ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã¾ã¨ã‚ã¦AIã«é€ä¿¡"""
        t_start = time.time()
        print(f"[1/4] èªè­˜å®Œäº† âœ“")
        print(f"  YOU: {user_text}")
        if self.on_english_text:
            self.on_english_text(user_text)

        # 3. AI ã«è³ªå•
        print(f"[2/4] AI å¿œç­”å¾…ã¡ ({self.ai_chat.model})...")
        self._notify_status("AI å¿œç­”ä¸­...")
        t_step = time.time()
        try:
            ai_response = self.ai_chat.chat(user_text)
        except Exception as e:
            print(f"[2/4] AI ã‚¨ãƒ©ãƒ¼: {e}")
            return
        t_ai = time.time() - t_step

        if not ai_response.strip():
            print("[2/4] (ç©ºå¿œç­”ã‚¹ã‚­ãƒƒãƒ—)")
            self._notify_status("ãƒã‚¤ã‚¯å¾…æ©Ÿä¸­...")
            return

        print(f"[2/4] AI å¿œç­”å®Œäº† ({t_ai:.1f}s) âœ“")
        print(f"  AI:  {ai_response}")
        if self.on_japanese_text:
            self.on_japanese_text(ai_response)

        # ãƒ­ã‚°ä¿å­˜
        print("[3/4] ãƒ­ã‚°ä¿å­˜ä¸­...")
        self.logger.log(
            "user", "ai",
            user_text, ai_response,
        )
        print("[3/4] ãƒ­ã‚°ä¿å­˜å®Œäº† âœ“")

        # 4. éŸ³å£°åˆæˆï¼ˆãšã‚“ã ã‚‚ã‚“ç­‰ã§èª­ã¿ä¸Šã’ï¼‰
        print("[4/4] éŸ³å£°åˆæˆä¸­...")
        self._notify_status("éŸ³å£°åˆæˆä¸­...")
        t_step = time.time()
        try:
            audio_path = self.tts.synthesize(ai_response)
        except Exception as e:
            print(f"[4/4] TTS ã‚¨ãƒ©ãƒ¼: {e}")
            return
        t_tts = time.time() - t_step

        if audio_path:
            self.player.enqueue(audio_path)

        t_total = time.time() - t_start
        print(f"[4/4] éŸ³å£°åˆæˆå®Œäº† ({t_tts:.1f}s) âœ“")
        print(f"[====] åˆè¨ˆ {t_total:.1f}s (AI{t_ai:.1f}s + TTS{t_tts:.1f}s)")
        self._notify_latency(t_total,
            f"AI{t_ai:.1f}s+TTS{t_tts:.1f}s")

        print("[====] ãƒã‚¤ã‚¯å¾…æ©Ÿä¸­... è©±ã—ã‹ã‘ã¦ãã ã•ã„")
        self._notify_status("ãƒã‚¤ã‚¯å¾…æ©Ÿä¸­...")

    def chat_text(self, text: str):
        """ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‹ã‚‰ AI ãƒãƒ£ãƒƒãƒˆï¼ˆGUI ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ç”¨ï¼‰"""
        if not self.ai_chat or not text.strip():
            return

        def _process():
            print(f"[YOU] {text}")
            if self.on_english_text:
                self.on_english_text(text)

            self._notify_status("AI å¿œç­”ä¸­...")
            try:
                ai_response = self.ai_chat.chat(text)
            except Exception as e:
                print(f"[Chat] AI ã‚¨ãƒ©ãƒ¼: {e}")
                self._notify_status("ãƒã‚¤ã‚¯å¾…æ©Ÿä¸­..." if self._running else "åœæ­¢ä¸­")
                return

            if not ai_response.strip():
                return

            print(f"[AI] {ai_response}")
            if self.on_japanese_text:
                self.on_japanese_text(ai_response)

            # ãƒ­ã‚°ä¿å­˜
            self.logger.log("user", "ai", text, ai_response)

            # éŸ³å£°åˆæˆ
            self._notify_status("éŸ³å£°åˆæˆä¸­...")
            try:
                audio_path = self.tts.synthesize(ai_response)
                if audio_path:
                    self.player.enqueue(audio_path)
            except Exception as e:
                print(f"[Chat] TTS ã‚¨ãƒ©ãƒ¼: {e}")

            self._notify_status("ãƒã‚¤ã‚¯å¾…æ©Ÿä¸­..." if self._running else "åœæ­¢ä¸­")

        threading.Thread(target=_process, daemon=True).start()

    def _notify_status(self, status: str):
        if self.on_status_change:
            self.on_status_change(status)

    def start(self):
        """ç¿»è¨³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹"""
        if self._running:
            return

        self._running = True
        self.capture.start()
        self.player.start()
        self._pipeline_thread = threading.Thread(target=self._pipeline_loop, daemon=True)
        self._pipeline_thread.start()
        print("[VoiceBridge] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")

    def stop(self):
        """ç¿»è¨³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åœæ­¢"""
        self._running = False
        self.capture.stop()
        self.player.stop()
        self.tts.cleanup()
        self.logger.close()

        if self._pipeline_thread:
            self._pipeline_thread.join(timeout=3.0)
            self._pipeline_thread = None

        print("[VoiceBridge] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢")

    def change_model(self, model_size: str):
        self.transcriber.change_model(model_size)

    def change_device(self, device_name: str):
        was_running = self._running
        if was_running:
            self.capture.stop()
        self.capture.device_name = device_name
        if was_running:
            self.capture.start()

    def change_voice(self, voice_key: str):
        """å£°ã‚’å¤‰æ›´ã™ã‚‹ï¼ˆEdge TTS ã®å ´åˆã¯ã‚­ãƒ¼åã€VOICEVOX ã®å ´åˆã¯ speaker_idï¼‰"""
        if self.use_voicevox:
            try:
                speaker_id = int(voice_key)
                self._voicevox_speaker_id = speaker_id
                if isinstance(self.tts, VoicevoxTTS):
                    self.tts.set_speaker(speaker_id)
            except ValueError:
                print(f"[VoiceBridge] ç„¡åŠ¹ãª speaker_id: {voice_key}")
        else:
            self.tts.set_voice(voice_key)

    def change_language_pair(self, source: str, target: str) -> bool:
        """è¨€èªãƒšã‚¢ã‚’å‹•çš„ã«å¤‰æ›´"""
        # Transcriber ã®è¨€èªå¤‰æ›´
        if not self.transcriber.set_language(source):
            return False

        # Translator ã®è¨€èªãƒšã‚¢å¤‰æ›´
        if not self.translator.set_language_pair(source, target):
            return False

        # TTS ã®è¨€èªå¤‰æ›´ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã«åˆã‚ã›ã‚‹ï¼‰
        # VOICEVOX ä½¿ç”¨ä¸­ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒæ—¥æœ¬èªä»¥å¤– â†’ Edge TTS ã«åˆ‡ã‚Šæ›¿ãˆ
        if self.use_voicevox and isinstance(self.tts, VoicevoxTTS) and target != "ja":
            self.tts.cleanup()
            self.tts = TTSEngine(language=target)
            print(f"[VoiceBridge] TTS: VOICEVOX â†’ Edge TTS ({target})")
        # VOICEVOX ãŒåˆ©ç”¨å¯èƒ½ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒæ—¥æœ¬èªã«æˆ»ã£ãŸ â†’ VOICEVOX ã«å¾©å¸°
        elif self.use_voicevox and not isinstance(self.tts, VoicevoxTTS) and target == "ja":
            self.tts.cleanup()
            self.tts = VoicevoxTTS(speaker_id=self._voicevox_speaker_id)
            print(f"[VoiceBridge] TTS: Edge TTS â†’ VOICEVOX")
        else:
            if not self.tts.set_language(target):
                return False

        # å†…éƒ¨çŠ¶æ…‹ã‚’æ›´æ–°
        self.source_language = source
        self.target_language = target
        self.tts_language = target

        print(f"[VoiceBridge] è¨€èªãƒšã‚¢ã‚’ {source}â†’{target} ã«å¤‰æ›´")
        return True


def run_cli(args):
    """CLI ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ"""
    use_voicevox = VoicevoxTTS.is_available()
    bridge = VoiceBridge(
        device_name=args.device,
        model_size=args.model,
        source_language=args.source_lang,
        target_language=args.target_lang,
        tts_language=args.tts_lang,
        voice=args.voice,
        chunk_duration=args.chunk,
        use_voicevox=use_voicevox,
        voicevox_speaker_id=args.speaker_id if use_voicevox else 3,
        asr_engine=args.asr,
        mode=args.mode,
        ai_base_url=args.ai_base_url,
        ai_model=args.ai_model,
    )

    # Ctrl+C ã§åœæ­¢
    def signal_handler(sig, frame):
        print("\n[CLI] åœæ­¢ä¸­...")
        bridge.stop()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)

    tts_name = "VOICEVOX" if use_voicevox else "Edge TTS"
    os_name = "Windows (WASAPI)" if IS_WINDOWS else "macOS (BlackHole)"
    asr_name = "Moonshine" if args.asr == "moonshine" else f"faster-whisper ({args.model})"
    mode_name = "AI ãƒãƒ£ãƒƒãƒˆ" if args.mode == "chat" else "ç¿»è¨³"
    print("=" * 50)
    print(f"  Voice Bridge - CLI ãƒ¢ãƒ¼ãƒ‰ï¼ˆ{mode_name}ï¼‰")
    print(f"  OS: {os_name}")
    print(f"  ASR: {asr_name}")
    if args.mode == "chat":
        print(f"  AI: {args.ai_model}")
    print(f"  ãƒ‡ãƒã‚¤ã‚¹: {args.device}")
    print(f"  TTS: {tts_name}")
    print(f"  ãƒãƒ£ãƒ³ã‚¯: {args.chunk}ç§’")
    print("  Ctrl+C ã§åœæ­¢")
    print("=" * 50)

    # CLI éŸ³å£°ãƒ¬ãƒ™ãƒ«è¡¨ç¤º
    def on_cli_level(rms: float, is_active: bool):
        bar_len = int(min(rms * 200, 30))
        bar = "â–ˆ" * bar_len + "â–‘" * (30 - bar_len)
        marker = " ğŸ¤" if is_active else ""
        print(f"\r  [{bar}] {rms:.3f}{marker}  ", end="", flush=True)

    bridge.on_level = on_cli_level

    bridge.start()

    # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚‚å—ã‘ä»˜ã‘ã‚‹
    if args.mode == "chat":
        print("  ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚‚å¯èƒ½ã§ã™ï¼ˆEnter ã§é€ä¿¡ï¼‰")
        print("=" * 50)

    # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç”Ÿã‹ã—ã¦ãŠã
    try:
        if args.mode == "chat":
            # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰: ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚‚å—ã‘ä»˜ã‘ã‚‹
            while True:
                try:
                    user_input = input()
                    if user_input.strip():
                        bridge.chat_text(user_input.strip())
                except EOFError:
                    break
        else:
            while True:
                time.sleep(0.5)
    except KeyboardInterrupt:
        bridge.stop()


def run_gui(args):
    """GUI ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ"""
    from gui import VoiceBridgeGUI

    # åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹ä¸€è¦§ã‚’å–å¾—
    try:
        devices = [d["name"] for d in AudioCapture.list_devices()]
    except Exception:
        devices = [DEFAULT_DEVICE]

    # VOICEVOX ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª
    voicevox_available = VoicevoxTTS.is_available()
    voicevox_speakers = {}
    if voicevox_available:
        voicevox_speakers = VoicevoxTTS.fetch_speakers()
        print(f"[VoiceBridge] VOICEVOX æ¤œå‡º: {len(voicevox_speakers)}è©±è€…")
    else:
        print("[VoiceBridge] VOICEVOX æœªæ¤œå‡º â†’ Edge TTS ã‚’ä½¿ç”¨")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® speaker_idï¼ˆãšã‚“ã ã‚‚ã‚“ ãƒãƒ¼ãƒãƒ«ï¼‰
    default_speaker_id = 3

    bridge = VoiceBridge(
        device_name=args.device,
        model_size=args.model,
        source_language=args.source_lang,
        target_language=args.target_lang,
        tts_language=args.tts_lang,
        voice=args.voice,
        chunk_duration=args.chunk,
        use_voicevox=voicevox_available,
        voicevox_speaker_id=default_speaker_id,
        asr_engine=args.asr,
        mode=args.mode,
        ai_base_url=args.ai_base_url,
        ai_model=args.ai_model,
    )

    # å£°å¤‰æ›´ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    def on_voice_change(voice_key: str):
        if voicevox_available:
            sid = voicevox_speakers.get(voice_key)
            if sid is not None:
                bridge.change_voice(str(sid))
                # VOICEVOX åˆ©ç”¨è¡¨è¨˜ã‚’æ›´æ–°ï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’åæ˜ ï¼‰
                char_name = voice_key.split("ï¼ˆ")[0]
                gui.set_credit(f"VOICEVOX:{char_name} | https://voicevox.hiroshiba.jp/")
            else:
                print(f"[GUI] ä¸æ˜ãªè©±è€…: {voice_key}")
        else:
            bridge.change_voice(voice_key)

    # è¨€èªãƒšã‚¢å¤‰æ›´ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    def on_language_pair_change(source: str, target: str):
        bridge.change_language_pair(source, target)

    gui = VoiceBridgeGUI(
        on_start=bridge.start,
        on_stop=bridge.stop,
        on_clear=None,
        on_model_change=bridge.change_model,
        on_device_change=bridge.change_device,
        on_voice_change=on_voice_change,
        on_language_pair_change=on_language_pair_change,
    )

    # GUI ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    bridge.on_english_text = gui.add_english_text
    bridge.on_japanese_text = gui.add_japanese_text
    bridge.on_status_change = gui.set_status
    bridge.on_level = gui.set_level
    bridge.on_latency = gui.set_latency

    # å£°ã®ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
    if voicevox_available:
        voice_list = list(voicevox_speakers.keys())
        default_voice = "ãšã‚“ã ã‚‚ã‚“ï¼ˆãƒãƒ¼ãƒãƒ«ï¼‰" if "ãšã‚“ã ã‚‚ã‚“ï¼ˆãƒãƒ¼ãƒãƒ«ï¼‰" in voice_list else voice_list[0]
    else:
        voice_list = ["nanamiï¼ˆå¥³æ€§ï¼‰", "keitaï¼ˆç”·æ€§ï¼‰"]
        default_voice = "nanamiï¼ˆå¥³æ€§ï¼‰"

    gui.build(
        devices=devices,
        voices=voice_list,
        default_voice=default_voice,
        default_source_lang=args.source_lang,
        default_target_lang=args.target_lang,
    )

    # VOICEVOX åˆ©ç”¨è¡¨è¨˜ï¼ˆåˆ©ç”¨è¦ç´„ã«åŸºã¥ãã‚¯ãƒ¬ã‚¸ãƒƒãƒˆè¡¨è¨˜ï¼‰
    if voicevox_available:
        credit = f"VOICEVOX:{default_voice.split('ï¼ˆ')[0]} | https://voicevox.hiroshiba.jp/"
        gui.set_credit(credit)

    gui.run()


def main():
    parser = argparse.ArgumentParser(
        description="Voice Bridge - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¤šè¨€èªç¿»è¨³",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("--cli", action="store_true", help="CLI ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰")
    parser.add_argument("--list-devices", action="store_true", help="å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ä¸€è¦§ã‚’è¡¨ç¤º")
    parser.add_argument("--asr", default="whisper", choices=["whisper", "moonshine"],
                        help="ASR ã‚¨ãƒ³ã‚¸ãƒ³ (default: whisper)")
    parser.add_argument("--device", default=DEFAULT_DEVICE,
                        help=f"å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹å (default: {DEFAULT_DEVICE})")
    parser.add_argument("--model", default="small", choices=["tiny", "base", "small", "medium"],
                        help="Whisper ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆmoonshine ä½¿ç”¨æ™‚ã¯ç„¡è¦–ï¼‰")
    parser.add_argument("--source-lang", default="en",
                        choices=["en", "ja", "zh", "es", "fr", "de", "ko"],
                        help="èªè­˜è¨€èª (default: en)")
    parser.add_argument("--target-lang", default="ja",
                        choices=["en", "ja", "zh", "es", "fr", "de", "ko"],
                        help="ç¿»è¨³è¨€èª (default: ja)")
    parser.add_argument("--tts-lang", default=None,
                        choices=["en", "ja", "zh", "es", "fr", "de", "ko"],
                        help="éŸ³å£°åˆæˆè¨€èª (default: target-lang ã¨åŒã˜)")
    parser.add_argument("--voice", default="nanami", choices=["nanami", "keita", "jenny", "guy", "xiaoxiao", "yunxi", "elvira", "alvaro", "denise", "henri", "katja", "conrad", "sunhi", "injoon"],
                        help="Edge TTS éŸ³å£° (VOICEVOXæœªä½¿ç”¨æ™‚)")
    parser.add_argument("--speaker-id", type=int, default=3,
                        help="VOICEVOX speaker ID (default: 3 = ãšã‚“ã ã‚‚ã‚“)")
    parser.add_argument("--chunk", type=float, default=4.0, help="éŸ³å£°ãƒãƒ£ãƒ³ã‚¯é•·ï¼ˆç§’ï¼‰")

    # AI ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
    parser.add_argument("--mode", default="translate", choices=["translate", "chat"],
                        help="å‹•ä½œãƒ¢ãƒ¼ãƒ‰: translateï¼ˆç¿»è¨³ï¼‰/ chatï¼ˆAIä¼šè©±ï¼‰")
    parser.add_argument("--ai-base-url", default=None,
                        help="AI API ãƒ™ãƒ¼ã‚¹ URL (default: .env ã® AI_BASE_URL or OpenAI)")
    parser.add_argument("--ai-model", default=None,
                        help="AI ãƒ¢ãƒ‡ãƒ«å (default: .env ã® AI_MODEL or gpt-4o-mini)")

    args = parser.parse_args()

    # .env / ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è£œå®Œ
    if args.ai_base_url is None:
        args.ai_base_url = os.environ.get("AI_BASE_URL", "https://api.openai.com/v1")
    if args.ai_model is None:
        args.ai_model = os.environ.get("AI_MODEL", "gpt-4o-mini")

    if args.list_devices:
        print("åˆ©ç”¨å¯èƒ½ãªå…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹:")
        for d in AudioCapture.list_devices():
            extra = ""
            if d.get("is_loopback"):
                extra = " [LOOPBACK]"
            print(f"  [{d['index']}] {d['name']} (ch={d['channels']}){extra}")
        return

    if args.cli:
        run_cli(args)
    else:
        run_gui(args)


if __name__ == "__main__":
    main()
